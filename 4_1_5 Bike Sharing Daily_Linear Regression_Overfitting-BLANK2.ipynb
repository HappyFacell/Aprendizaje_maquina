{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 4.1.5 Bike Sharing Daily, Regression, Gradient Descent, Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo de regresión es un procedimiento estadístico que permite al investigador estimar la relación linel que relacion dos o más variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- El algoritmo de programacion para implementar el modelo de regresión lineal es el siguiente: \n",
    "\n",
    "    1. Cargar el dataset, normalizar y dividir los datos en datos de entrenamiento y datos de prueba, añadir la columna de unos para w0.\n",
    "    2. Inicializar W, y calcular el gradiente de W\n",
    "    3. Mientras el gradiente sea más grande que epsilon, calcular:\n",
    "        - El gradiente para el valor actual de W\n",
    "        - Actualizar el valor para el siguiente W\n",
    "        - Cacular el costo y almacenarlo en una variable\n",
    "    4. Obtener la función de costo o error para el último valor de W\n",
    "    5. Predecir la salida con los datos de prueba y obtener el valor del error. \n",
    "\n",
    "- Para comprobar el Overfitting realizar el algoritmo de GD con 9 tamaños de datos de prueba: 0.1, 0.2,..., 0.9, y graficar el costo.\n",
    "\n",
    "- ¿Con qué tamaño de prueba (test_size) tendremos overfitting y con cuál underfiting?\n",
    "- Finalmente utilizar el test size que se eligió y hacer regresión polinomial de grado 1 al 5 y graficar el comportamiento del costo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cargar el dataset, normalizar y dividir los datos en datos de entrenamiento y datos de prueba, añadir la columna de unos para w0.\n",
    "\n",
    "Para este caso utilizaremos los datos del ejercicio de las Bicis, el dataset de horas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import norm\n",
    "from numpy.random import rand\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Load the dataset and convert to numpy arrays\n",
    "\n",
    "def loadCsv(filename):\n",
    "    data = pd.read_csv(filename)\n",
    "    dataset = np.array(data)\n",
    "    m, n = np.shape(dataset)\n",
    "    x = dataset[:, 0:n-1]\n",
    "    y = dataset[:, -1]\n",
    "    y = np.reshape(y ,(m,1))\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "#Testing\n",
    "filename = 'bike-day.csv'\n",
    "x, y = loadCsv(filename)\n",
    "print('X size:',np.shape(x), 'Y size:',  np.shape(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizar el dataset\n",
    "\n",
    "def normalize(x):\n",
    "    x_norm = (x - np.mean(x, axis=0)) / (np.ndarray.std(x, axis=0)) \n",
    "    return x_norm\n",
    "\n",
    "X_norm = normalize(x)\n",
    "\n",
    "print(np.shape(X_norm))\n",
    "print(X_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split datasets into training and testing\n",
    "def splitDataset(x, y,test_size):\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size = test_size, random_state = 1)\n",
    "    return xTrain, xTest, yTrain, yTest\n",
    "\n",
    "#Testing\n",
    "test_size = 0.33\n",
    "xTrain, xTest, yTrain, yTest = splitDataset(X_norm, y,test_size)\n",
    "\n",
    "print('Split X', len(x), 'rows into train with', len(xTrain), 'and test with', len(xTest))\n",
    "print('Split Y', len(y), 'rows into train with', len(yTrain), 'and test with', len(yTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding column 1 to the X matrix\n",
    "\n",
    "def addOnes(X):\n",
    "    X1=np.array(X)\n",
    "    m , n = np.shape(X1)\n",
    "    ones = np.ones((m, 1))\n",
    "    X1 = np.concatenate((ones, X1), axis=1)\n",
    "    return X1\n",
    "\n",
    "xTr = addOnes(xTrain)\n",
    "xTe = addOnes(xTest)\n",
    "print(np.shape(xTr))\n",
    "print(np.shape(xTe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inicializar W, y calcular el gradiente de W\n",
    "\n",
    "\n",
    "El gradiente es: $$ \\nabla J(W) = \\frac{\\partial J(W) }{\\partial W} = \\frac{\\partial}{\\partial W} (Y - XW)^2 = -2X(Y-XW)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize W using random values\n",
    "m, n = np.shape(xTr)\n",
    "print(m,n)\n",
    "initw = np.random.rand(1, n)\n",
    "print(initw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating Gradient\n",
    "\n",
    "def gradient(X, Y, W):\n",
    "    residual = Y - np.dot(X,W.T)\n",
    "    grad = -2 * np.dot(X.T, residual)\n",
    "    return grad\n",
    "\n",
    "#Testing gradient function\n",
    "\n",
    "Gradiente = gradient(xTr, yTrain, initw)\n",
    "print(Gradiente)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Iteración del Gradiente Descendente \n",
    "\n",
    "    Mientras la norma del gradiente sea más grande que epsilon, calcular:\n",
    "    \n",
    "       - El gradiente para el valor actual de W\n",
    "       - Actualizar el valor siguiente de W\n",
    "       - Almacenar el valor del error o del costo para ese W\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero definiremos la función que evalua el error utilizando MSE: $$MSE(w) = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - x_i w)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cost Function\n",
    "\n",
    "def mse(Y, Yt):\n",
    "    residual = Y - Yt\n",
    "    cost = np.dot(residual.T,residual) / len(Y)\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Descent with epsilon using the number of iterations\n",
    "\n",
    "def GD(X, Y, W, alpha, epsilon, iterations):\n",
    "    grad = gradient(X, Y, W)\n",
    "    gradNorm = np.linalg.norm(grad)\n",
    "    Yt = np.dot(X,W.T)\n",
    "    cost = mse(Y,Yt)\n",
    "    it = 0\n",
    "    J = [] #Lista donde guardaremos el valor del error (MSE) en cada iteración\n",
    "    \n",
    "    while gradNorm > epsilon and it < iterations:\n",
    "        \n",
    "        #calcular gradiente\n",
    "        grad = gradient(X, Y, W)\n",
    "        gradNorm = np.linalg.norm(grad)\n",
    "        \n",
    "        #Actualizar W\n",
    "        W = W - alpha * grad.T\n",
    "        \n",
    "        #Incrementar contador de iteraciones\n",
    "        it += 1\n",
    "        \n",
    "        #Calcular la predicción y el error (MSE)\n",
    "        Yt = np.dot(X, W.T)\n",
    "        cost  = mse(Y,Yt)\n",
    "\n",
    "        #Guardar el vector del error\n",
    "        J.append(float(cost))\n",
    "        \n",
    "    return W, it, J\n",
    "\n",
    "#Testing GD(X, Y, W, alpha, epsilon, iterations)\n",
    "\n",
    "w, iterations, J = GD(xTr, yTrain, initw, 0.00001, 0.01, 1000)\n",
    "\n",
    "print('W', w)\n",
    "print('Iterations', iterations)\n",
    "print('Error final ', J[-1])\n",
    "plt.plot(J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Medir el MSE del algoritmo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt = np.dot(xTr, w.T)\n",
    "costo = mse(yTrain,yt)\n",
    "\n",
    "print(np.shape(yTrain), np.shape(yt))\n",
    "print('Error (Costo) final: ', costo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Predecir los datos de prueba y calcular su error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt = np.dot(xTe, w.T)\n",
    "costo = mse(yTest,yt)\n",
    "\n",
    "print(np.shape(yTrain), np.shape(yt))\n",
    "print(costo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Overfitting y Underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crear un array que contenga los de tamaños de testing de 0 a 0.9, con un incremento de 0.1\n",
    "\n",
    "\n",
    "print(testsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Por cada elemento del array de tamaños hacer una prueba del Gradiente Descendiente \n",
    "#2. Almacenar los últimos valores de los costos (errores) del Training y del Testing en listas diferentes\n",
    "#3. Graficar los costos almacenados con respecto al vector testsize\n",
    "\n",
    "#Inicialización de parámetros:\n",
    "\n",
    "#Learning rate o índice de aprendizaje\n",
    "alpha = \n",
    "\n",
    "#Precisión (qué tan cercano a cero debe estar el error)\n",
    "epsilon = \n",
    "\n",
    "#Número máximo de iteraciones (es útil para que el algoritmo no se cicle)\n",
    "itera = \n",
    "\n",
    "#Incialización de listas para guardar los valores finales de los costos\n",
    "costosTraining=[]\n",
    "costosTest=[]\n",
    "\n",
    "#Cargar el archivo de nuevo y obtener matrices X y Y\n",
    "filename = \n",
    "x, y = \n",
    "print('X size:',np.shape(x), 'Y size:',  np.shape(y))\n",
    "\n",
    "#Para cada tamaño de testing hacer el gradiente descendente\n",
    "for tsize in testsize:\n",
    "    \n",
    "    #split X y Y\n",
    "    xTrain, xTest, yTrain, yTest = \n",
    "    \n",
    "    #Acomodar dimensiones de yTrain y YTest\n",
    "    yTrain = \n",
    "    yTest = \n",
    "    \n",
    "    #Añadir Unos a xTrain y xTest \n",
    "    xTr = \n",
    "    xTe = \n",
    "    \n",
    "    #Initizalize w de manera aleatoria\n",
    "    m , n = \n",
    "    initw = \n",
    "    \n",
    "    #Obtener W a través del Gradiente Descendente\n",
    "    print(\"Test size: \",tsize)\n",
    "    \n",
    "    w, iterations, J = \n",
    "    \n",
    "    #Calcular el error (costo) de Train y Test\n",
    "    costoTrain = \n",
    "    costoTest = \n",
    "    print('Costo train: ', costoTrain)\n",
    "    print('Costo test: ', costoTest)\n",
    "    \n",
    "    #Guardar costos en las listas\n",
    "    \n",
    "    \n",
    "#Convertir a arrays las listas de los costos\n",
    "costosTraining = \n",
    "costosTest = \n",
    "\n",
    "#Acomodar sus dimensiones para poder graficar\n",
    "costosTraining = \n",
    "costosTest = \n",
    "\n",
    "#Graficar los costos con respecto a testsize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Add Polynomial features\n",
    "\n",
    "- Para observar el Underfitting y el Overfitting de estos datos hacer un modelo para cada grado polinomial hasta grado 5.  \n",
    "\n",
    "    - Prueba 1: $ X = [x]$\n",
    "    - Prueba 2: $ X = [x \\quad x^2]$\n",
    "    - Prueba 3: $ X = [x \\quad x^2 \\quad x^3]$\n",
    "    - Prueba 4: $ X = [x \\quad x^2 \\quad x^3 \\quad x^4]$\n",
    "    - Prueba 5: $ X = [x \\quad x^2 \\quad x^3 \\quad x^4 \\quad x^5]$\n",
    "\n",
    "- De cada prueba obtener sus W's correspondientes y su error MSE (Costo)\n",
    "\n",
    "- Graficar el MSE de cada prueba con respecto al grado utilizado\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Incializar parámetros\n",
    "alpha = \n",
    "epsilon = \n",
    "itera = \n",
    "seed = #Semilla para funciones aleatorias\n",
    "\n",
    "#Incializar listas para guardar los costos finales\n",
    "costosTraining = []\n",
    "costosTest = []\n",
    "\n",
    "#Tamaño del testing size, observar celda anterior\n",
    "tsize =\n",
    "\n",
    "#Cargar de nuevo el archivo y obtener X y Y\n",
    "filename = \n",
    "x, y = \n",
    "print('X size:',np.shape(x), 'Y size:',  np.shape(y))\n",
    "\n",
    "#Variable para formar matrices polinomiales\n",
    "xtemp = x\n",
    "\n",
    "#Grados que voy a evaluar de 1 a 5\n",
    "poly = \n",
    "\n",
    "#Para cada grado polinomial realizar el gradiente descendente, obtener W's y calcular el MSE (error o costo)\n",
    "for grade in poly:\n",
    "    print(\"Grade: \",grade) \n",
    "    \n",
    "    # Construir Matriz polinomial de grado \"grade\"\n",
    "    \n",
    "        \n",
    "    # Añadir unos a la matriz polinomial\n",
    "    x_1s = \n",
    "    \n",
    "    # Split dataset en Training and Testing\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(x_1s, y, test_size = tsize , random_state = seed)\n",
    "    yTrain = yTrain.reshape(-1,1);\n",
    "    yTest = yTest.reshape(-1,1);\n",
    "    \n",
    "    # Incializar W de forma aleatoria\n",
    "    \n",
    "    print('W: ',w.shape)\n",
    "\n",
    "    # Obtener W y el error MSE (J) a través del GD\n",
    "    w, it, J = GD(xTrain, yTrain, w, alpha, epsilon, itera)\n",
    "    \n",
    "    #Imprimir la iteración it\n",
    "    \n",
    "    #Calcular errores MSE de Training y Testing\n",
    "    costoTrain = \n",
    "    costoTest =     \n",
    "    \n",
    "    print('Training cost:', costoTrain)\n",
    "    print('Testing cost:', costoTest)\n",
    "    \n",
    "    #Guardar los valores en las listas\n",
    "    costosTraining.append(costoTrain)\n",
    "    costosTest.append(costoTest)\n",
    "    \n",
    "#Convertir a arrays las listas\n",
    "costosTraining = np.array(costosTraining)\n",
    "costosTest = np.array(costosTest)\n",
    "\n",
    "#Adecuar dimensiones\n",
    "costosTraining = costosTraining.reshape(len(costosTraining), 1)\n",
    "costosTest = costosTest.reshape(len(costosTest), 1)\n",
    "\n",
    "#Graficar los errores con respecto al grado\n",
    "plt.plot(poly, costosTraining, 'b')\n",
    "plt.plot(poly, costosTest, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escribir conclusiones y observaciones del ejercicio."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
